\chapter{Markov链与决策}\label{chap:markov-chain}

\begingroup
\newcommand{\pref}{Chapters/Markov-chain/figures}

\renewcommand{\P}{\mathcal P}
\newcommand{\MS}{\mathcal S}
\newcommand{\M}{\mathcal M}

\section{Markov链}

我们在第一次课说过，似然，或者说合情推理的合理程度，是一种概率的解释模型. 尽管概率论在数学上通常被形式化为Kolmogorov公理体系，但是公理体系并没有回答``概率''是什么. 概率的解释是一个哲学课题. 两个主要的例子：
\begin{itemize}
\item \emph{频率解释}\index{频率解释}：概率是无穷次独立重复试验的频率（大数定律）.
\item \emph{主观解释}（\emph{Bayes解释}）\index{主观解释}\index{Bayes解释}：概率是对命题合理程度的信念（似然）.
\end{itemize}

主观解释对推理的假设是逻辑的、静态的，时间的概念并不出现在似然里面. 例如，考虑一个罐子，里面有除颜色之外不可区分的$N$个球，有$n$个白球，剩下的是黑球. 顺序从中拿出$N$个球，第$k$次拿出的球颜色是$W_k$或$B_k$.
\begin{itemize}
    \item $\Pr(W_iW_j)=\Pr(W_i|W_j)\Pr(W_j)=\Pr(W_j|W_i)\Pr(W_i)$.（$i<j$）
    \item $\Pr(W_i)=\Pr(W_j)=n/N\implies\Pr(W_i|W_j)=\Pr(W_j|W_i)$.
\end{itemize}
从似然的角度，$\Pr(W_i|W_j)$和$\Pr(W_j|W_i)$不仅是可计算的，而且是相等的. 概率的计算告诉了我们，更早状态的信息依赖于未来的状态！逻辑上蕴含关系并不意味着实际上的因果关系，但是似然完全没有考虑这一点. 因此，我们需要引入一个带有时间的模型，这就是Markov链.

\begin{definition}[Markov链]
\textbf{Markov链}\index{Markov链}\index{马氏链}（\textbf{马氏链}）是一个随机变量序列$\{X_t\}_{t=0}^{\infty}$. 包含如下概念：
\begin{itemize}
	\item 状态空间\index{状态空间} $\MS$：$X_t$所有可能值构成的集合，有限或者可数.
	\item 转移矩阵\index{转移矩阵} $\P$：下一时刻系统状态之间转移的概率. $\P=(p_{ij})_{i,j\in \MS}$，$p_{ij}$是从$i$状态转移到$j$状态的概率.
	\item Markov性\index{Markov性}: 对任意时刻$t=1,\dots,n$和任意状态$j,k,j_0,\dots,j_{t-1}\in \MS$，如下等式成立
		\begin{align*}
		   &\Pr(X_{t+1}=j| X_t=k,X_{t-1}=j_{t-1},\dots,X_0=j_0)\\
		   =& \Pr(X_{t+1}=j| X_{t}=k)=p_{kj}.
		\end{align*}
    \end{itemize}
    有时候也会考虑带初态的Markov链，此时$X_0$服从分布$\lambda=(\lambda_s)_{s\in \MS}$.
\end{definition}
我们给出的定义是简化的Markov链，每个时刻之间的转移都是一样的转移矩阵，这样的Markov链被称为\emph{时齐的}\index{时齐的}. 有时候也会考虑非时齐的Markov链，即每个时刻之间的转移矩阵不一样.

Markov链是一种简化的带时间的概率模型，它最重要的性质是Markov性，即在固定现在的情况下，过去与未来相互独立. 这一性质的数学表述为：
\begin{proposition}[Markov性]\index{Markov性}\label{prop:markov}
条件在$X_n=i$下，$\{Y_m\}_{m=0}^{\infty}:=\{X_{m+n}\}_{m=0}^{\infty}$是一个转移矩阵为$P$的Markov链，并且与$(X_0,\dots,X_{n-1})$相互独立. % HW: 证明这个命题
\end{proposition}
证明留做习题. 

我们考虑的Markov链还有时齐性，即状态的转移不依赖当前时间，只和当前的状态有关. 时齐性的数学表述为：
\begin{proposition}
    设$\{X_t\}_{t=0}^{\infty}$是一个Markov链，那么对任意的$t,m,n\in \N$和$i,j,k\in \MS$，有
    $\Pr(X_{m+n}=j| X_{n}=k)=\Pr(X_{m}=j| X_{0}=k)$.
\end{proposition}

我们来看一个Markov链的例子. 
\begin{example}[赌徒模型]\index{赌徒模型}
    考虑公平对赌. 玩家$A$和$B$抛硬币来赌钱，$A$赌正面，$B$赌反面. 每一轮独立地抛硬币，正面朝上的概率和反面朝上的概率相等，都是$1/2$. 赢的一方给输的一方一块钱. $A$输$a$块钱破产，$B$输$b$块钱破产，$Z_i$是第$i$轮$A$的收入. $Z_0=X_0=0$是$A$初始的收入. $X_n=Z_0+\dots+Z_n$是$A$的累计收入. 那么，$\{X_n\}_{n\geq 0}$是一个Markov链.
    \begin{itemize}
        \item 状态空间：$\MS=\{-a,-a+1,\dots,0,1,\dots,b\}$.
        \item 转移概率：对$-a<i<b-1$，$p_{i,i+1}=p_{i+1,i}=1/2$；$p_{-a+1,-a}=p_{b-1,b}=1/2$，$p_{-a,-a}=p_{b,b}=1$；其他值为$0$.
    \end{itemize}
    转移矩阵可以画成\Cref{fig:gambling}所示的形式.
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{\pref/gampling.eps}
    \caption{赌徒模型的转移矩阵}\label{fig:gambling}
    \end{figure}
\end{example}

在上面的赌徒模型中，$A$的累计收入$\{X_n\}_{n\geq 0}$形成了Markov链. 根据Markov性，未来双方的收入变化只取决于现在，而和过去运气无关. 与之相关的一个现象是赌徒谬误\index{赌徒谬误}，即认为过去的运气会影响未来的运气. 例如，如果一个人连续输了很多次，那么他会认为自己未来运气会变好，赢的概率更大. 但是，根据Markov性，过去的运气不会影响未来的运气，因此这种想法是错误的. ``风水轮流转''在一场公平对赌中是不正确的认知. 那么，如何评估赌局的公平性？

如果对赌是公平的，那么我们应该认为两个人每一轮的累计收入分布都是一样的，即
    \[\Pr(X_n=i|X_0=0)=\Pr(X_n=-i|X_0=0).\]
因此，我们需要能够计算多步转移的概率. 设$p_{ij}^{(k)}$表示从状态$i$用$k$步转移到状态$j$的概率. $k$步转移概率形成了一个矩阵$\P^{(k)}$. 下面的定理给出了计算\emph{多步转移概率}的方法.

\begin{theorem}[Kolmogorov-Chapman方程]\label{thm:kolmogorov-chapman}\index{Kolmogorov-Chapman方程}
    $P^{(k+l)}=\P^{(k)}\P^{(l)}$.
\end{theorem}

\begin{proof}
由Markov性、时齐性和全概率公式，$p_{ij}^{(k+l)}=\Pr(X_{k+l}=j|X_0=i)=\sum_{\alpha}\Pr(X_{k+l}=j,X_k=\alpha|X_0=i)=\sum_{\alpha}\Pr(X_k=\alpha|X_0=i)\Pr(X_{k+l}=j|X_k=\alpha)=\sum_{\alpha} p_{i\alpha}^{(k)}p_{\alpha j}^{(l)}$.
\end{proof}

Kolmogorov-Chapman方程有两个重要的特例，前向方程：$\P^{(k+1)}=\P^{(k)}\P$，以及后向方程：$\P^{(l+1)}=\P\P^{(l)}$. 见\Cref{fig:forward-equation} 和\Cref{fig:backward-equation}.
\begin{figure}[ht]
    \begin{minipage}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{\pref/forward-equation.eps}
        \caption{前向方程（往前一步）}
        \label{fig:forward-equation}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{\pref/backward-equation.eps}
        \caption{后向方程（往回一步）}
        \label{fig:backward-equation}
    \end{minipage}
\end{figure}

此外，利用归纳法，我们还有如下推论：
\begin{corollary}\label{cor:kolmogorov-chapman}
    $\P^{(k)}=\P^k$.    
\end{corollary}

若已知初始分布向量为$\lambda$，利用这一推论，我们可以计算它随时间的演化：
	\[\lambda^\t,\lambda^\t \P,\dots,\lambda^\t \P^n,\dots\] %HW: 证明这个演化

回到赌徒模型，如何计算公平对赌中$X_n$的概率分布？我们先来看一个简化的例子. 假设$|p_{00}+p_{11}-1|<1$，考虑只有两个状态$0,1$，转移矩阵为
	\[\P=\begin{pmatrix}p_{00}&p_{01}\\p_{10}&p_{11}
	\end{pmatrix}.\]
或者画成\Cref{fig:simple-example} 的形式.
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{\pref/simple-example.eps}
    \caption{只有两个状态的Markov链}
    \label{fig:simple-example}
\end{figure}

可以归纳证明：
\begin{align*}
    \P^n=&\frac{1}{2-p_{00}-p_{11}}\begin{pmatrix}1-p_{11}&1-p_{00}\\1-p_{11}&1-p_{00}\end{pmatrix}\\
    &+\frac{(p_{00}+p_{11}-1)^n}{2-p_{00}-p_{11}}\begin{pmatrix}1-p_{00}&-(1-p_{00})\\-(1-p_{11})&1-p_{11}\end{pmatrix}.
\end{align*}

注意到，$\lim_{n\to\infty}p_{i0}^{(n)}=(1-p_{11})/(2-p_{00}-p_{11})$，$\lim_{n\to\infty}p_{i1}^{(n)}=(1-p_{00})/(2-p_{00}-p_{11})$. 随着时间的推移，Markov链初始状态对概率分布的影响逐渐消失. 这个规律具有普遍性，这就是\emph{遍历定理}.


\begin{theorem}[遍历定理]\label{thm:ergodic-theorem}\index{遍历定理}
    设Markov链的状态空间为$\MS=\{1,\dots,N\}$，转移矩阵为$\P=(p_{ij})$. 如果对于某一个$n_0$有
    \begin{equation}
        \min_{ij}p_{ij}^{(n_0)}>0,\label{eq:reachable}
    \end{equation}
    那么存在分布$\lambda=(\lambda_1,\dots,\lambda_N)$使得
    \begin{equation}
        \lambda_i>0,\quad\sum_i\lambda_i=1,\label{eq:positive-distribution}
    \end{equation}
    并且对于每一个$j\in\MS$和任意$i\in\MS$都有
    \begin{equation}
    p_{ij}^{(n)}\to\lambda_j,n\to\infty.\label{eq:converge-to-limit}
    \end{equation}
    反之，如果存在满足 \eqref{eq:positive-distribution} 和 \eqref{eq:converge-to-limit} 的$\lambda$，则存在满足 \eqref{eq:reachable} 的$n_0$.
    式 \eqref{eq:positive-distribution} 的$\lambda$满足
    \begin{equation}
        \lambda^\t = \lambda^\t\P.\label{eq:stationary}
    \end{equation}
\end{theorem}

条件 \eqref{eq:reachable} 表明超过某个步数$n_0$之后，从$i$出发到达$j$的概率总是正的，这个条件被称为\emph{遍历}\index{遍历}. 条件 \eqref{eq:positive-distribution} 表明每一个状态被访问到的概率都是正的，没有``死状态''. 遍历定理表明遍历的Markov链从任何状态出发都是不可逆的，最终会把每个状态都走过一遍（遍历），变成一个混合均匀的状态. 这可以用来解释物理学中的扩散现象. % HW: 算一下Ehrenfest模型

满足条件 \eqref{eq:stationary} 的分布被称为\emph{平稳分布}\index{平稳分布}. 平稳分布为初始状态时，Markov链的演化与时间无关：
\begin{proposition}\label{prop:stationary-distribution}
设$\{X_n\}$是Markov链，如果$X_0$是平稳分布，那么$(X_k,\dots,X_{k+l})$的联合分布不依赖于$k$.
\end{proposition}

如果Markov链是遍历的，那么平稳分布是唯一的：
\begin{proposition}\label{prop:unique-stationary-distribution}
设$\{X_n\}$是遍历的Markov链，那么它有唯一平稳分布$\mu$. 
\end{proposition}
    \begin{proof}
        假设$\mu$是另外一个平稳分布，那么$\mu_j=\sum_\alpha\mu_\alpha p_{\alpha j}=\dots=\sum_{\alpha}\mu_\alpha p_{\alpha j}^{(n)}$. 因为$p_{\alpha j}^{(n)}\to \lambda_j$，所以$\mu_j=\sum_{\alpha} (\mu_\alpha\lambda_j)=\lambda_j$.
    \end{proof}

非遍历Markov链也可能存在（唯一）平稳分布，考虑如下转移矩阵：
\[\P=\begin{pmatrix}0&1\\1&0\end{pmatrix},\]
它有唯一平稳分布$\lambda=(1/2,1/2)^\t$.


\section{Markov奖励过程（MRP）}\index{Markov奖励过程}\index{MRP}

我们接下来的目标就是在Markov链上建立决策理论. 每一阶段我们可以选择某个行动，这个行动在Markov链会产生一些奖励. 我们的目标是选择恰当的行动方式是的我们的总奖励最大. 首先我们定义奖励的过程.

\begin{definition}
一个\textbf{Markov奖励过程}（MRP）\index{Markov奖励过程}\index{MRP}是四元组$\langle\MS,\P,\mathcal R,\gamma\rangle$：
\begin{itemize}
    \item $\MS$是一个有穷的状态集合.
    \item $\P$是一个状态转移矩阵，从$i$转移到$j$的概率记为$\P_{i,j}$.
    \item $\mathcal R$是一个奖励函数，$\mathcal R_s = \E[\textcolor{red}{R_{t+1}}|S_t=s]$：当$t$时刻位于状态$s$时 下一时刻（离开）获得的奖励的期望，$\textcolor{red}{R_{t+1}}$是下一阶段所处状态的奖励.
    \item $\gamma$是一个折扣系数，$\gamma\in[0,1]$.
\end{itemize}
\end{definition}


接下来我们看一个例子：学生MRP. 
\begin{example}[学生MRP]
见\Cref{fig:student-MRP}.\lhysays{补全}
\begin{figure}[ht]
    \centering
    \includegraphics[height=0.4\textheight]{\pref/STR.eps}
    \caption{学生MRP}
    \label{fig:student-MRP}
\end{figure}
\end{example}

 MRP中，$t$时刻以后的总\emph{回报}\index{回报}$G_t$定义为
    \[G_t = R_{t+1}+\gamma R_{t+2} +\dots =\sum_{k=0}^\infty \gamma^kR_{t+k+1}.\]
$\gamma \in[0,1]$衡量了未来下一时段1的奖励在当前时刻的价值. 未来$k+1$时刻的奖励对当前时刻$t$的作用是$\gamma^k R_{t+k+1}$. 若$\gamma\to0$，表示对奖励进行“短视”的评估；反之更“远见”.

许多MRP和后面学习的MDP都有与时间无关的折扣系数$\gamma <1$，原因例如，对未来不确定性对冲，这直接对英语直接对应于利润率. 另外，动物和人类对即时回报具有偏好. 有时也使用非折扣化的MRP（即$\gamma=1$），例如当所有的转移序列都会有固定的终止时间.

在MRP中，状态\emph{价值函数}\index{价值函数}$v(s)$表示从状态$s$出发的期望回报
    \[v(s) = \E(G_t|S_t=s).\]
价值函数$v(s)$衡量了状态$s$的长期效益. 这一定义蕴含了Markov性：只从当前起考虑未来收益，不考虑历史收益（沉没成本）的影响；也蕴含了时齐性：价值函数的定义不依赖于时刻$t$（无穷阶段情形）.

价值函数可以被分解为两部分：即时回报$\textcolor{red}{R_{t+1}}$以及下一个状态开始的折扣价值 $\textcolor{green}{\gamma v(S_{t+1})}$，具体来说，我们有
\begin{align*}
v(s) &= \E(G_t|S_t=s) \\
    &= \E(R_{t+1}+ \gamma R_{t+2} + \gamma^2 R_{t+3}+\dots | S_t= s) \\
    &= \E(R_{t+1} + \gamma (R_{t+2}+\gamma R_{t+3}+\dots) | S_t = s) \\
    &= \E(R_{t+1} + \gamma G_{t+1} | S_t = s) \\
    &= \E(\textcolor{red}{R_{t+1}} + \textcolor{green}{\gamma v(S_{t+1})}| S_t=s)\\
    &= {\textcolor{red}{\mathcal R_s} + \textcolor{green}{\gamma \sum_{s'\in \MS}\P_{s,s'}v(s')}}. % HW: 做一下破产博弈的Wald等式. 看一下zfx应随教材1.10三（100页），可以在题目里直接假设停时概率1有限
\end{align*}
我们因此得到了\textbf{Bellman方程}\index{Bellman方程}：
\begin{theorem}[Bellman方程]\index{Bellman方程}
    $v(s)=\mathcal R_s + \gamma \sum_{s'\in \MS}\P_{s,s'}v(s')$.
\end{theorem}

Bellman方程可以用矩阵形式表达：
        \[v = \mathcal R + \gamma \mathcal P v.\]
这里$v$是列向量$v=(v(s))_{s\in\MS}$.

Bellman方程是一个线性方程，可以被直接解:
\[
    v =\mathcal R + \gamma \P v \implies (I-\gamma \P)v = \mathcal R \implies v = (I-\gamma \P)^{-1} \mathcal R.
\]

对于$n$个状态的Markov链，计算复杂度为$\O(n^3)$. 对于较小的MRP可以直接解，太大的MRP开销太大. 对于大型MRP，可以采用迭代算法，例如：
\begin{itemize}
    \item \index{动态规划}动态规划
    \item \index{Monte-Carlo评估}Monte-Carlo评估
    \item \index{时序差分学习}时序差分学习
\end{itemize}


\section{Markov决策过程（MDP）}\index{Markov决策过程}\index{MDP}
接下来我们定义Markov决策过程. MDP是MRP的扩展，它增加了\emph{行动}的概念，是一个定义了决策的MRP. 它可以看做一个任意状态都具有Markov性的\emph{环境}.

\begin{definition}
\textbf{Markov决策过程}（MDP）\index{Markov决策过程}\index{MDP}是一个MDP是五元组$\langle\MS, \textcolor{green}{\mathcal A}, \P, \mathcal R, \gamma\rangle$，其中
\begin{itemize}
    \item $\MS$是一个有限的状态集合.
    \item $\textcolor{green}{\mathcal A}$是一个有限的\emph{行动}（action）集合.
    \item $\P$是状态转移概率矩阵，
    \[\P_{ss'}^{\textcolor{green}{a}} = \Pr(S_{t+1} = s' | S_t = s, A_t = \textcolor{green}{a}).\]
    \item $\mathcal R$是一个奖励函数，$\mathcal R_s^{\textcolor{green}{a}} = \E(\textcolor{red}{R_{t+1}} | S_t = s, A_t = \textcolor{green}{a})$，$\textcolor{red}{R_{t+1}}$是进行某一行动到达某一状态后的奖励.
    \item $\gamma$是一个折扣系数$\gamma\in[0,1]$.
\end{itemize}
\end{definition}


我们继续前面学生的例子，此时变成学生MDP：
\begin{example}[学生MDP]
见\Cref{fig:studentMDP}.
\begin{figure}
    \centering
    \includegraphics[height=0.4\textheight]{\pref/STD.eps}
    \caption{学生MDP}
    \label{fig:studentMDP}
\end{figure}
\end{example}

一个\emph{策略}$\pi$是给定状态下行动的分布，
    \[\pi(a|s) = \Pr(A_t=a | S_t = s).\]
一个策略完全决定了一个智能体在MDP环境中的行为. 它蕴含着Markov性：MDP的策略取决于当前状态，而非历史状态；也蕴含着时齐性：MDP的策略不依赖于时刻$t$.

MDP与Markov链、MDP的关系由策略给出. 
\begin{proposition}
给定一个MDP $\M=\langle\MS,\mathcal A,\P,\mathcal R, \gamma\rangle$和一个策略$\pi$，$\langle \MS, \P^{\pi}\rangle$是一个Markov链，$\langle\MS,\P^{\pi}, \mathcal R^{\pi}, \gamma\rangle$是一个MRP，其中
\[\P_{s,s'}^{\pi} = \E_{a\sim\pi(\cdot|s)}(\P^a_{s,s'})=\sum_{a\in \mathcal A}\pi(a|s)\mathcal P_{s,s'}^{a},\]
    \[\mathcal R_s^{\pi} =\E_{a\sim\pi(\cdot|s)}(\mathcal R^a_s)=\sum_{a\in\mathcal A}\pi(a|s)\mathcal R_s^a.\]
\end{proposition}
\begin{proof}
利用全概率公式. 
\end{proof}

在MDP中，\emph{状态-价值函数}\index{状态-价值函数}和\emph{行动-价值函数}\index{行动-价值函数}是两个重要的价值函数，它们分别描述了从某一状态出发，遵从某一策略的期望回报. 
状态-价值函数$v_\pi(s)$是从状态$s$出发，遵从策略$\pi$的期望回报
    \[v_\pi(s) = \E_\pi(G_t|S_t=s).\]
行动-价值函数$q_\pi(s,a)$是从状态$s$出发，采取行动$a$，遵从策略$\pi$的期望回报
    \[q_\pi(s,a) = \E_\pi(G_t|S_t=s,A_t=a).\]
注意，以上定义都具有Markov性和时齐性.

下面我们给出状态-价值函数和行动-价值函数的Bellman方程. 
状态-价值函数可以被分解为：\emph{即时回报}加\emph{后续状态的折扣价值}，
\[v_\pi(s) = \E_\pi(R_{t+1} + \gamma v_\pi(S_{t+1})|S_t=s).\]

行动-价值函数可以被类似地分解，
\[q_\pi(s,a) = \E_\pi(R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a).\]

二者之间的关系（全概率公式、一步转移概率）：
\[q_\pi(s,a) =\mathcal R_s^a + \gamma \sum_{s'\in \MS}P_{s,s'}^a v_\pi(s').\]
\[v_\pi(s) = \E_{a\sim\pi(\cdot|s)}(q_\pi(s,a))=\sum_{a\in\mathcal A}\pi(a|s)q_\pi(s,a),\]
因此，我们得到MDP的Bellman期望方程：

\begin{proposition}[状态-价值函数和行动-价值函数的Bellman方程]\index{Bellman方程}
\[v_\pi(s) = \sum_{a\in \mathcal A}\pi(a|s)\left(\mathcal R_s^a + \gamma \sum_{s'\in \MS}\P_{s,s'}^av_\pi(s')\right),\]
\[q_\pi(s,a) = \mathcal R_s^a + \gamma \sum_{s'\in \MS}\P_{s,s'}^a \sum_{a'\in \mathcal A}\pi(a'|s')q_\pi(s',a').\]
\end{proposition}

状态-价值函数的Bellman方程可以被写成矩阵形式：
\[v_\pi = \mathcal R^\pi + \gamma \P^\pi v_\pi = (I-\gamma \mathcal P^\pi)^{-1}\mathcal R^\pi.\]

接下来我们讨论最优策略和最优价值函数. 

\begin{definition}[最优状态-价值函数和最优行动-价值函数]\index{最优状态-价值函数}\index{最优行动-价值函数}
\textbf{最优状态-价值函数} $v_\star(s)$ 是所有决策中最大的状态-价值函数
    \[v_\star(s) = \max_\pi v_\pi(s).\]

\textbf{最优行动-价值函数} $q_\star(s,a)$是所有决策中最大的行动-价值函数
    \[q_\star(s,a) = \max_\pi q_\pi(s,a).\]
\end{definition}
最优价值函数确定了MDP中的最佳收益，解MDP即确定达到最优价值函数的策略. 

然而，每个状态取到最大价值的策略$\pi$可能并不是同一个. 幸运的是，确实存在一个这样的最优策略. 定义一个策略的偏序：
    \[\pi\ge\pi' \iff \forall s\in\MS\ v_\pi(s) \ge v_{\pi'}(s).\]
我们有如下定理：
\begin{theorem}[MDP解的存在性]
对任意MDP，
\begin{itemize}
    \item 存在一个最优策略 $\pi_\star$使得$\forall \pi\ \pi_\star\ge\pi$.
    \item 最优策略取得最优状态-价值函数：$v_{\pi_\star}(s) = v_\star(s)$.
    \item 最优策略取得最优行动-价值函数：$q_{\pi_\star}(s,a)=q_\star(s,a)$.
\end{itemize}
\end{theorem}

\begin{proof}
我们给出一个构造性证明，即找出最优决策. 可以通过最大化$q_\star(s,a)$来寻找：
    \begin{itemize}
        \item 固定$s$.
        \item 找到一个$a_\star$使得$q_\star(s,a_\star)=\max_{a}q_\star(s,a)$，令$\pi_\star(a_\star|s)=1$.
        \item 对$\forall a\neq a_\star$，$\pi_\star(a|s)=0$.
    \end{itemize}
首先，根据选法，$\pi_\star$取得最优行动-价值函数. 由$v_\pi(s) = \E_{a\sim\pi(\cdot|s)}(q_\pi(s,a))\leq \E_{a\sim\pi(\cdot|s)}(q_\star(s,a))\leq q_\star(s,a_\star)=v_{\pi_\star}(s)$知$\pi_\star$取得最优状态-价值函数.
\end{proof}
这个证明还有一个推论：
\begin{corollary}
    对任意MDP，总存在一个非随机的最优决策.    
\end{corollary}

 如果我们知道$q_\star(s,a)$，我们就能获得最优决策. 最优价值函数由Bellman最优性方程联系：
\[v_\star(s) = \max_a q_\star(s,a),\]
\[q_\star(s,a) = \mathcal R_s^a + \gamma \sum_{s'\in \MS}\P_{s,s'}^av_\star(s'),\]
\[v_\star(s) = \max_a\left\{\mathcal R_s^a + \gamma \sum_{s'\in \MS}\P_{s,s'}^av_\star(s')\right\},\]
\[q_\star(s,a) = \mathcal R_s^a+\gamma \sum_{s'\in \MS}\P_{s,s'}^a\max_{a'}q_\star(s',a').\]

Bellman最优性方程不是线性的. 因此没有解析形式的解. 但是MDP的数值解是可以多项式时间求出来的. 我们一般采用迭代算法求解：
\begin{itemize}
    \item 价值迭代\index{价值迭代}
    \item 策略迭代\index{策略迭代}
    \item Q-learning\index{Q-learning}
    \item Sarsa\index{Sarsa}
\end{itemize}


Bellman方程是\emph{\index{强化学习}强化学习}、经济学\emph{\index{动态优化}动态优化}的核心. Bellman方程的推导是Markov链中最为常用的技巧：考虑从当前状态转移到下一状态，利用全概率公式，一步转移会将两个状态之间的概率（期望）用递推公式联系起来. 在随机过程中，有大量这样的例子：\emph{\index{前向方程}前向方程}、\emph{\index{Wald等式}Wald等式}、\emph{\index{调和函数}调和函数}. 后面的\emph{\index{HMM}HMM}也是类似的例子.


\section{隐Markov模型（HMM）}\index{隐Markov模型}\index{HMM}

我们考虑Markov链上的另一种应用. 在统计学和机器学习中，我们有时候要处理一类含时间的数据. 最简单的情况是回归，即数据完全由所处时刻决定. 但是通常，现在的数据依赖于过去的数据. 因此，一种最简单的考虑就是数据依赖于Markov链，这就是\emph{隐Markov模型}.

\begin{definition}[隐Markov模型]
一个\textbf{隐Markov模型}（HMM）\index{隐Markov模型}\index{HMM} 是一列随机变量$X_1,X_2,\dots, X_t$，满足：
    \begin{itemize}
        \item $X_t$的分布仅依赖于隐状态$Z_t$，即$\Pr(X_1,\dots,X_t|Z_1,Z_2,\dots,Z_t)=\prod_i \Pr(X_i|Z_i)$.
        \item $\{Z_t\}$构成一条Markov链.
    \end{itemize}
\end{definition}
示意见\Cref{fig:HMM}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{\pref/HMM.eps}
    \caption{隐Markov模型}
    \label{fig:HMM}
\end{figure}    



我们可以更具体地说写出来HMM的结构. 一个HMM包含：
    \begin{itemize}
        \item $\mathcal Z$
        : 有限的状态集合.
        \item $\mathcal X$: 有限的观测集合.
        \item $T: \mathcal Z\times\mathcal Z\to \R_{\geq 0}$，$\mathcal Z$的转移概率.
        \item $M:\mathcal Z\times \mathcal X\to \R_{\geq 0}$，给定状态时的观测概率（条件概率）.
        \item $\lambda:\mathcal Z\to\R_{\geq 0}$，初始状态的先验概率分布列.
    \end{itemize}
如果随机过程$\{X_t\}$的值域是有限集，我们则可以用矩阵表达HMM：
    \begin{itemize}
    \item $T$是$\{Z_t\}$的转移矩阵.
    \item $M$是观测矩阵：$M_{i,k} = \Pr(X_t=k|Z_t=i)$.
    \item $\lambda$是一个概率向量.
    \end{itemize}


\subsection{评估问题}

给定一个特定的HMM，它对实际观测序列的拟合程度有多好？为了讨论，我们引入记号随机向量$X=(X_1,\dots,X_t)$，$Z=(Z_1,\dots,Z_t)$. 我们考虑HMM的\emph{评估}问题：给定一个HMM $\M$，以及它的观测历史$x=(x_1,x_2,\dots,x_t)$，计算$\Pr(X=x|\M)$. 关键困难是我们不知道状态历史$Z=(z_1,z_2,\dots,z_t)$.

直接使用条件概率进行推导，我们可以得到如下算法：
    \[
        \Pr(X=x|\M) =\sum_{Z=(z_1, \dots, z_t) \in \mathcal Z} \Pr(X=x|Z=z, \M)\Pr(Z=z|\M),
    \]
    \[
        \Pr(X=x|Z=z,\M) = \prod_{i=1}^t \Pr(X_i = x_i| Z_i = z_i) = M_{z_1,x_1}\cdot M_{z_2,x_2}\dots M_{z_t,x_t},
    \]
    \begin{align*}
        \Pr(Z=z|\M) &= \Pr(Z_1 = z_1) \prod_{i=2}^t\Pr(Z_i = z_i| Z_{i-1} = z_{i-1}) 
        \\
        &= \lambda_{z_1}\cdot T_{z_1,z_2}\cdot T_{z_2,z_3}\dots T_{z_{t-1},z_t}.
    \end{align*}
这一方法的时间复杂度是$\O(t|\mathcal Z|^t)$. 然而，这一算法中，$t$在指数上，因此是不可接受的. 我们需要更好的算法.

接下来，我们采用前向方程的思路，从前$k$步的结果推出前$k+1$步的结果. 因此可以列出递推方程. 为了方便，引入记号：$X_{i:j}=(X_i,\dots,X_j)$. 然后，定义$\alpha_k(z):= \Pr(X_{1:k}=x_{1:k}, Z_k=z| \M)$，我们有递推：
    \begin{itemize}
        \item $\alpha_1(z) = \lambda(z)M_{z,x_1}$.
        \item $\alpha_{k+1}(z) = \sum_{z' \in \mathcal Z}\alpha_{k}(z')T_{z',z}M_{z,x_{k+1}}$.
    \end{itemize}
于是，$\Pr(X=x| \M) = \sum_{z\in \mathcal Z}\alpha_t(z)$.

这一方法的时间复杂度是$\mathcal O(t|\mathcal Z|^2)$.

镜像地，我们可以使用后向方程的思路，从前$k+1$步的结果推出前$k$步的结果. 同样可以列出递推方程. 定义$\beta_k(z):=\Pr(X_{k+1:t}=x_{k+1:t} | Z_k=z,\M)$，我们有递推：
\begin{itemize}
    \item 当$k = t$，$\beta_k(z) = 1$.
    \item 当$1 \le k < t$，$\beta_{k}(z) = \sum_{z' \in \mathcal Z}T_{z,z'}M_{z',x_{k+1}}\beta_{k+1}(z')$.
\end{itemize}
于是，$\Pr(X=x| \M) = \sum_{z\in \mathcal Z}\lambda(z)M_{z,x_1}\beta_1(z)$.
这一方法的时间复杂度是时间复杂度 $O(t|\mathcal Z|^2)$.


\subsection{解释问题}
接下来我们讨论HMM的\emph{解释问题}. 给定一个 HMM $\M = (\mathcal Z, \mathcal X, T, M, \lambda)$, 一列观测历史$x = (x_1, x_2, \dots, x_t)$, 解释问题旨在寻找一个状态序列，能最好地解释这些历史观察. 具体来说我们考虑如下四个问题：
\begin{enumerate}
    \item 过滤：计算$\Pr(Z_k = s|X_{1:k}=x_{1:k}, \M)$.
    \item 平滑：计算$\Pr(Z_k = s|X=x, \M)$，$k < t$.
    \item 预测：计算$\Pr(Z_k = s|X=x, \M)$，$k > t$.
    \item 解码：找到最有可能的状态序列 $z = (z_1, z_2, \dots, z_t)$.
\end{enumerate}


首先考虑过滤：$\Pr(Z_k = s|X_{1:k}=x_{1:k}, \M)$. 回顾记号$\alpha_k(s)= \Pr(X_{1:k}=x_{1:k}, Z_k=s| \M)$，我们有
    \begin{align*}
        \Pr(Z_k = s|X_{1:k}=x_{1:k}, \M) & = \frac{\Pr(X_{1:k}=x_{1:k}, Z_k=s| \M)}{\Pr(X_{1:k}=x_{1:k}| \M)} \\
        &= \frac{\alpha_k(s)}{\sum_{z\in\mathcal Z}\alpha_k(z)}.
    \end{align*}
这一推导给出了一个计算过滤的算法.

然后是平滑：$\Pr(Z_k = s|X=x, \M)$，$k < t$.  回顾记号$\alpha_k(s)= \Pr(X_{1:k}=x_{1:k}, Z_k=s| \M)$，以及$\beta_k(s)=\Pr(X_{k+1:t}=x_{k+1:t} |Z_k=s, \M)$. 可以证明：
        \[\Pr(z_k = s|X=x, \M)=\frac{\beta_k(s)\alpha_k(s)}{\sum_{z\in\mathcal Z}\alpha_t(z)}.
    \]
这一推导给出了一个计算平滑的算法.


之后是预测：$\Pr(Z_k = s|X=x, \M)$，$k > t$. 首先用过滤计算 $\lambda=\Pr(Z_t = s|X=x, \M)$. 然后用 $\lambda$ 作为Markov的初始状态，利用Kolmogorov-Chapman方程向前计算$k-t$步.


最后是解码. 定义 
    $$\delta_k(s) = \max_{Z_{1:k-1}}\Pr(Z_{1:k} = (z_{1:k-1}, s), X_{1:k}=x_{1:k}| \M).$$
根据一步转移，我们有
    $$\delta_{k+1}(s) = \max_{q\in \mathcal Z}\{\delta_k(q)T_{q,s}\}M_{s,x_{k+1}}.$$
问题转化为: 记录最高概率的路径，这是一个动态规划问题.


\newcommand{\pre}{\mathrm{Pre}}
我们有\emph{Viterbi算法}\index{Viterbi算法}，如下：
\begin{itemize}
    \item 初始化:
    \begin{itemize}
        \item $\delta_1(s) = \lambda(s)M_{s,z_1}$.
        \item $\pre_1(s) = \varnothing$.
    \end{itemize}
    \item 对 $k=1, 2, \dots, t-1$，$s \in \mathcal Z$：
    \begin{itemize}
        \item $\delta_{k+1}(s) = \max_{q\in \mathcal Z}\{\delta_k(q)T_{q,s}\}M_{s,x_{k+1}}$.
        \item $\pre_{k+1}(s) = \argmax_{q\in \mathcal Z}\{\delta_k(q)T_{q,s}\}$.
    \end{itemize}
    \item $z_t = \argmax_{s \in \mathcal Z}\delta_{t}(s)$.
    \item 对 $1 \le k < t$，$z_k = \pre_{k+1}(z_{k+1})$.
    \item 时间复杂度: $\mathcal O(t|\mathcal Z|^2)$.
\end{itemize}

\endgroup