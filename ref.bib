@article{breimanIndividualErgodicTheorem1957,
  title = {The {{Individual Ergodic Theorem}} of {{Information Theory}}},
  author = {Breiman, Leo},
  year = {1957},
  journal = {The Annals of Mathematical Statistics},
  volume = {28},
  number = {3},
  eprint = {2237247},
  eprinttype = {jstor},
  pages = {809--811},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  urldate = {2023-07-10}
}

@book{coverElementsInformationTheory2012,
  title = {Elements of {{Information Theory}}},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = {2012},
  publisher = {{John Wiley \& Sons}},
  file = {/Users/lhy/Zotero/storage/IMJGJ7NL/books.html}
}

@article{huffmanMethodConstructionMinimumRedundancy1952,
  title = {A {{Method}} for the {{Construction}} of {{Minimum-Redundancy Codes}}},
  author = {Huffman, David A.},
  year = {1952},
  month = sep,
  journal = {Proceedings of the IRE},
  volume = {40},
  number = {9},
  pages = {1098--1101},
  issn = {2162-6634},
  doi = {10.1109/JRPROC.1952.273898},
  abstract = {An optimum method of coding an ensemble of messages consisting of a finite number of members is developed. A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized.},
  keywords = {Transmitters},
  file = {/Users/lhy/Zotero/storage/5MSAMRK6/4051119.html}
}

@misc{InformationEtymologyOrigin,
  title = {Information | {{Etymology}}, Origin and Meaning of Information by Etymonline},
  urldate = {2023-07-10},
  howpublished = {https://www.etymonline.com/word/information}
}

@book{jaynesProbabilityTheoryLogic2002,
  title = {Probability {{Theory}}: {{The Logic}} of {{Science}}},
  shorttitle = {Probability {{Theory}}},
  author = {Jaynes, Edwin T.},
  year = {2002},
  publisher = {{Cambridge University Press}},
  abstract = {The standard rules of probability can be interpreted as uniquely valid principles in logic. In this book, E. T. Jaynes dispels the imaginary distinction between 'probability theory' and 'statistical inference', leaving a logical unity and simplicity, which provides greater technical power and flexibility in applications. This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. New results are discussed, along with applications of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book will be of interest to scientists working in any area where inference from incomplete information is necessary.},
  googlebooks = {ZWVvkQEACAAJ},
  langid = {english}
}

@article{kullbackInformationSufficiency1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  eprint = {2236703},
  eprinttype = {jstor},
  pages = {79--86},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  urldate = {2023-07-10}
}

@misc{lewisBARTDenoisingSequencetoSequence2019,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  year = {2019},
  month = oct,
  number = {arXiv:1910.13461},
  eprint = {1910.13461},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-07-10},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lhy/Zotero/storage/XKHHJN94/Lewis 等 - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf;/Users/lhy/Zotero/storage/M8J7LCY9/1910.html}
}

@book{LiXianPingGaiLuLunJiChu2010,
  title = {{概率论基础}},
  author = {李贤平},
  year = {2010},
  publisher = {{高等教育出版社}},
  urldate = {2023-07-10},
  isbn = {978-7-04-028890-2},
  langid = {chinese}
}

@article{mcmillanBasicTheoremsInformation1953,
  title = {The {{Basic Theorems}} of {{Information Theory}}},
  author = {McMillan, Brockway},
  year = {1953},
  month = jun,
  journal = {The Annals of Mathematical Statistics},
  volume = {24},
  number = {2},
  pages = {196--219},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729028},
  urldate = {2023-07-10},
  abstract = {This paper describes briefly the current mathematical models upon which communication theory is based, and presents in some detail an exposition and partial critique of C. E. Shannon's treatment of one such model. It then presents a general limit theorem in the theory of discrete stochastic processes, suggested by a result of Shannon's.},
  file = {/Users/lhy/Zotero/storage/4S4XD6QM/McMillan - 1953 - The Basic Theorems of Information Theory.pdf}
}

@book{robertm.fanoTransmissionInformation1949,
  title = {The {{Transmission}} of {{Information}}},
  author = {{Robert M. Fano}},
  year = {1949},
  month = mar,
  urldate = {2023-07-10},
  abstract = {Fano's conception of information theory, including Shannon-Fano coding},
  langid = {english},
  keywords = {information theory}
}

@incollection{rumelhartLearningInternalRepresentations1986,
  title = {Learning Internal Representations by Error Propagation},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
  author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
  year = {1986},
  month = jan,
  pages = {318--362},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  urldate = {2023-07-09},
  isbn = {978-0-262-68053-0}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, C. E.},
  year = {1948},
  month = jul,
  journal = {The Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  file = {/Users/lhy/Zotero/storage/2QCADYFL/6773024.html}
}

@book{shiryaevProbability1996,
  title = {Probability},
  author = {Shiryaev, A. N.},
  year = {1996},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {95},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4757-2539-1},
  urldate = {2023-07-10},
  isbn = {978-1-4757-2541-4 978-1-4757-2539-1},
  keywords = {Ergodic theory,Markov chain,Martingale,Probability theory,Random variable,random walk,stochastic process,Stochastic processes},
  file = {/Users/lhy/Zotero/storage/VDSRGRMH/Shiryaev - 1996 - Probability.pdf}
}

@article{tingAmountInformation1962a,
  title = {On the {{Amount}} of {{Information}}},
  author = {Ting, Hu Kuo},
  year = {1962},
  month = jan,
  journal = {Theory of Probability \& Its Applications},
  volume = {7},
  number = {4},
  pages = {439--447},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0040-585X},
  doi = {10.1137/1107041},
  urldate = {2023-07-10},
  abstract = {In this paper it is proved that the condition on the information stability of a sequence of channels is not only sufficient, but also necessary for the validity of Feinstein's lemma and Shannon's theorem.}
}

@incollection{uffinkBoltzmannWorkStatistical2022,
  title = {Boltzmann's {{Work}} in {{Statistical Physics}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Uffink, Jos},
  editor = {Zalta, Edward N.},
  year = {2022},
  edition = {Summer 2022},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  urldate = {2023-07-10},
  abstract = {Ludwig Boltzmann (1844\textendash 1906) is generally acknowledged as one ofthe most important physicists of the nineteenth century. Particularlyfamous is his statistical explanation of the second law ofthermodynamics. The celebrated formula S=klogWS=klog⁡WS = k \textbackslash log W, expressing arelation between entropy SSS and probability WWW has been engravedon his tombstone (even though he never actually wrote this formuladown). Boltzmann's views on statistical physics continue to play animportant role in contemporary debates on the foundations of thattheory.},
  keywords = {{Boltzmann, Ludwig},{Mach, Ernst},physics: intertheory relations in,{probability, interpretations of},statistical physics: philosophy of statistical mechanics},
  file = {/Users/lhy/Zotero/storage/5788FCCK/statphys-Boltzmann.html}
}
