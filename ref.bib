@incollection{blumTHEORYCOMPUTATIONCOMPLEXITY2000,
  title = {{{ON A THEORY OF COMPUTATION AND COMPLEXITY OVER THE REAL NUMBERS}}: {{NP-COMPLETENESS}}, {{RECURSIVE FUNCTIONS AND UNIVERSAL MACHINES}}},
  shorttitle = {{{ON A THEORY OF COMPUTATION AND COMPLEXITY OVER THE REAL NUMBERS}}},
  year = {2000},
  month = jun,
  pages = {1293--1338},
  doi = {10.1142/9789812792839_0013},
  urldate = {2024-09-12},
  abstract = {We present a model for computation over the reals or an arbitrary (ordered) ring R. In this general setting, we obtain universal machines, partial recursive functions, as well as JVP-complete problems. While our theory reflects the classical over Z (e.g., the computable functions are the recursive functions) it also reflects the special mathematical character of the underlying ring R (e.g., complements of Julia sets provide natural examples of R. E. undecidable sets over the reals) and provides a natural setting for studying foundational issues concerning algorithms in numerical analysis.},
  langid = {english},
  file = {/Users/lhydave/Zotero/storage/GM64RSBG/Blum 等 - 2000 - ON A THEORY OF COMPUTATION AND COMPLEXITY OVER THE REAL NUMBERS NP-COMPLETENESS, RECURSIVE FUNCTION.pdf},
  author = {Blum, Lenore and Shub, Mike and Smale, Steve}
}

@article{breimanIndividualErgodicTheorem1957,
  title = {The {{Individual Ergodic Theorem}} of {{Information Theory}}},
  year = {1957},
  journal = {The Annals of Mathematical Statistics},
  volume = {28},
  number = {3},
  eprint = {2237247},
  eprinttype = {jstor},
  pages = {809--811},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851},
  urldate = {2023-07-10},
  author = {Breiman, Leo}
}

@book{coverElementsInformationTheory2012,
  title = {Elements of {{Information Theory}}},
  year = {2012},
  publisher = {John Wiley \& Sons},
  author = {Cover, Thomas M. and Thomas, Joy A.}
}

@article{delongPrimateModelsMovement1990,
  title = {Primate Models of Movement Disorders of Basal Ganglia Origin},
  year = {1990},
  month = jul,
  journal = {Trends in Neurosciences},
  volume = {13},
  number = {7},
  pages = {281--285},
  issn = {0166-2236},
  doi = {10.1016/0166-2236(90)90110-V},
  urldate = {2024-08-06},
  abstract = {Movement disorders associated with basal ganglia dysfunction comprise a spectrum of abnormalities that range from the hypokinetic disorders (of which Parkinson's disease is the best-known example) at one extreme to the hyperkinetic disorders (exemplified by Huntington's disease and hemiballismus) at the other. Both extremes of this movement disorder spectrum can be accounted for by postulating specific disturbances within the basal ganglia-thalamocortical `motor' circuit. In this paper, Mahlon DeLong describes the changes in neuronal activity in the motor circuit in animal models of hypo- and hyperkinetic disorders.},
  file = {/Users/lhydave/Zotero/storage/BXPNR5CA/016622369090110V.html},
  author = {DeLong, Mahlon R.}
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-02-28},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {/Users/lhydave/Zotero/storage/PDDC4I3A/Ho 等 - 2020 - Denoising Diffusion Probabilistic Models.pdf},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter}
}

@article{huffmanMethodConstructionMinimumRedundancy1952,
  title = {A {{Method}} for the {{Construction}} of {{Minimum-Redundancy Codes}}},
  year = {1952},
  month = sep,
  journal = {Proceedings of the IRE},
  volume = {40},
  number = {9},
  pages = {1098--1101},
  issn = {2162-6634},
  doi = {10.1109/JRPROC.1952.273898},
  abstract = {An optimum method of coding an ensemble of messages consisting of a finite number of members is developed. A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized.},
  keywords = {Transmitters},
  author = {Huffman, David A.}
}

@misc{InformationEtymologyOrigin,
  title = {Information {\textbar} {{Etymology}}, Origin and Meaning of Information by Etymonline},
  urldate = {2023-07-10},
  note = {(accessed 2023-07-10)},
  howpublished = {\url{https://www.etymonline.com/word/information}}
}

@book{jaynesProbabilityTheoryLogic2002,
  title = {Probability {{Theory}}: {{The Logic}} of {{Science}}},
  shorttitle = {Probability {{Theory}}},
  year = {2002},
  publisher = {Cambridge University Press},
  abstract = {The standard rules of probability can be interpreted as uniquely valid principles in logic. In this book, E. T. Jaynes dispels the imaginary distinction between 'probability theory' and 'statistical inference', leaving a logical unity and simplicity, which provides greater technical power and flexibility in applications. This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. New results are discussed, along with applications of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book will be of interest to scientists working in any area where inference from incomplete information is necessary.},
  googlebooks = {ZWVvkQEACAAJ},
  langid = {english},
  author = {Jaynes, Edwin T.}
}

@article{kullbackInformationSufficiency1951,
  title = {On {{Information}} and {{Sufficiency}}},
  year = {1951},
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  eprint = {2236703},
  eprinttype = {jstor},
  pages = {79--86},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851},
  urldate = {2023-07-10},
  author = {Kullback, S. and Leibler, R. A.}
}

@inproceedings{levinMarkovChainsMixing2008,
  title = {Markov {{Chains}} and {{Mixing Times}}},
  year = {2008},
  month = dec,
  publisher = {American Mathematical Society},
  address = {Providence, Rhode Island},
  doi = {10.1090/mbk/058},
  urldate = {2024-02-28},
  abstract = {This book is an introduction to the modern approach to the theory of Markov chains. The main goal of this approach is to determine the rate of convergence of a Markov chain to the stationary distribution as a function of the size and geometry of the state space. The authors develop the key tools for estimating convergence times, including coupling, strong stationary times, and spectral methods. Whenever possible, probabilistic methods are emphasized. The book includes many examples and provides brief introductions to some central models of statistical mechanics. Also provided are accounts of random walks on networks, including hitting and cover times, and analyses of several methods of shuffling cards. As a prerequisite, the authors assume a modest understanding of probability theory and linear algebra at an undergraduate level. ""Markov Chains and Mixing Times"" is meant to bring the excitement of this active area of research to a wide audience.},
  isbn = {978-0-8218-4739-8 978-1-4704-1204-3},
  langid = {english},
  author = {Levin, David and Peres, Yuval and Wilmer, Elizabeth}
}

@misc{lewisBARTDenoisingSequencetoSequence2019,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  year = {2019},
  month = oct,
  number = {arXiv:1910.13461},
  eprint = {1910.13461},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-07-10},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke}
}

@book{LiXianPingGaiLuLunJiChu2010,
  title = {{概率论基础}},
  year = {2010},
  publisher = {高等教育出版社},
  urldate = {2023-07-10},
  isbn = {978-7-04-028890-2},
  langid = {chinese},
  author = {李贤平}
}

@article{mcmillanBasicTheoremsInformation1953,
  title = {The {{Basic Theorems}} of {{Information Theory}}},
  year = {1953},
  month = jun,
  journal = {The Annals of Mathematical Statistics},
  volume = {24},
  number = {2},
  pages = {196--219},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729028},
  urldate = {2023-07-10},
  abstract = {This paper describes briefly the current mathematical models upon which communication theory is based, and presents in some detail an exposition and partial critique of C. E. Shannon's treatment of one such model. It then presents a general limit theorem in the theory of discrete stochastic processes, suggested by a result of Shannon's.},
  author = {McMillan, Brockway}
}

@book{robertm.fanoTransmissionInformation1949,
  title = {The {{Transmission}} of {{Information}}},
  year = {1949},
  month = mar,
  urldate = {2023-07-10},
  abstract = {Fano's conception of information theory, including Shannon-Fano coding},
  langid = {english},
  keywords = {information theory},
  author = {{Robert M. Fano}}
}

@incollection{rumelhartLearningInternalRepresentations1986,
  title = {Learning Internal Representations by Error Propagation},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
  year = {1986},
  month = jan,
  pages = {318--362},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  urldate = {2023-07-09},
  isbn = {978-0-262-68053-0},
  author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.}
}

@article{sellittoMyopicDiscountingFuture2010,
  title = {Myopic {{Discounting}} of {{Future Rewards}} after {{Medial Orbitofrontal Damage}} in {{Humans}}},
  year = {2010},
  month = dec,
  journal = {Journal of Neuroscience},
  volume = {30},
  number = {49},
  pages = {16429--16436},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2516-10.2010},
  urldate = {2024-08-04},
  abstract = {Choices are often intertemporal, requiring tradeoff of short-term and long-term outcomes. In such contexts, humans may prefer small rewards delivered immediately to larger rewards delivered after a delay, reflecting temporal discounting (TD) of delayed outcomes. The medial orbitofrontal cortex (mOFC) is consistently activated during intertemporal choice, yet its role remains unclear. Here, patients with lesions in the mOFC (mOFC patients), control patients with lesions outside the frontal lobe, and healthy individuals chose hypothetically between small-immediate and larger-delayed rewards. The type of reward varied across three TD tasks, including both primary (food) and secondary (money and discount vouchers) rewards. We found that damage to mOFC increased significantly the preference for small-immediate over larger-delayed rewards, resulting in steeper TD of future rewards in mOFC patients compared with the control groups. This held for both primary and secondary rewards. All participants, including mOFC patients, were more willing to wait for delayed money and discount vouchers than for delayed food, suggesting that mOFC patients' (impatient) choices were not due merely to poor motor impulse control or consideration of the goods at stake. These findings provide the first evidence in humans that mOFC is necessary for valuation and preference of delayed rewards for intertemporal choice.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2010 the authors 0270-6474/10/3016429-08\$15.00/0},
  langid = {english},
  pmid = {21147982},
  file = {/Users/lhydave/Zotero/storage/GV5CM4EB/Sellitto 等 - 2010 - Myopic Discounting of Future Rewards after Medial Orbitofrontal Damage in Humans.pdf},
  author = {Sellitto, Manuela and Ciaramelli, Elisa and di Pellegrino, Giuseppe}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A Mathematical Theory of Communication},
  year = {1948},
  month = jul,
  journal = {The Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  author = {Shannon, C. E.}
}

@book{shiryaevProbability1996,
  title = {Probability},
  year = {1996},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {95},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4757-2539-1},
  urldate = {2023-07-10},
  isbn = {978-1-4757-2541-4 978-1-4757-2539-1},
  keywords = {Ergodic theory,Markov chain,Martingale,Probability theory,Random variable,random walk,stochastic process,Stochastic processes},
  author = {Shiryaev, A. N.}
}

@inproceedings{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep Unsupervised Learning Using Nonequilibrium Thermodynamics},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  year = {2015},
  month = jul,
  series = {{{ICML}}'15},
  pages = {2256--2265},
  publisher = {JMLR.org},
  address = {Lille, France},
  urldate = {2024-02-27},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya}
}

@article{tingAmountInformation1962,
  title = {On the {{Amount}} of {{Information}}},
  year = {1962},
  month = jan,
  journal = {Theory of Probability \& Its Applications},
  volume = {7},
  number = {4},
  pages = {439--447},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0040-585X},
  doi = {10.1137/1107041},
  urldate = {2023-07-09},
  abstract = {In this paper it is proved that the condition on the information stability of a sequence of channels is not only sufficient, but also necessary for the validity of Feinstein's lemma and Shannon's theorem.},
  author = {Ting, Hu Kuo}
}

@incollection{uffinkBoltzmannWorkStatistical2022,
  title = {Boltzmann's {{Work}} in {{Statistical Physics}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  year = {2022},
  edition = {Summer 2022},
  publisher = {Metaphysics Research Lab, Stanford University},
  urldate = {2023-07-10},
  abstract = {Ludwig Boltzmann (1844--1906) is generally acknowledged as one ofthe most important physicists of the nineteenth century. Particularlyfamous is his statistical explanation of the second law ofthermodynamics. The celebrated formula S=klogWS=klog⁡WS = k {\textbackslash}log W, expressing arelation between entropy SSS and probability WWW has been engravedon his tombstone (even though he never actually wrote this formuladown). Boltzmann's views on statistical physics continue to play animportant role in contemporary debates on the foundations of thattheory.},
  keywords = {Boltzmann Ludwig,Mach Ernst,physics: intertheory relations in,probability interpretations of,statistical physics: philosophy of statistical mechanics},
  author = {Uffink, Jos},
  editor = {Zalta, Edward N.}
}
